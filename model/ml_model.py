# -*- coding: utf-8 -*-
"""ML Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XWrX-4SW-t3ZcMl57cw1_2TT984Ksv_N
"""

import nltk
# nltk.download('wordnet')
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer
import numpy as np
from tensorflow.keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD
import pandas as pd
import pickle
import random
import json

"""Importing our dataset"""

with open('intents.json') as json_data:
    intents = json.load(json_data)

"""Bulding our list of words, classes and words to tags mapping(documents)"""

words = []
classes = []
documents = []
ignore_words = ['?',',','.']

for intent in intents['intents']:
    for pattern in intent['patterns']:
        w = nltk.word_tokenize(pattern)
        # add to our words list
        words.extend(w)
        # add to documents in our corpus
        documents.append((w, intent['tag']))
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

# lemmatize and lower each word
lemma_function = WordNetLemmatizer()
words = [lemma_function.lemmatize(w.lower()) for w in words if w not in ignore_words]

# remove duplicates
words = sorted(list(set(words)))
# sort classes
classes = sorted(list(set(classes)))

# words = all words, vocabulary
print (len(words), "lemmatized words", words)
# documents = combination between patterns and intents
print (len(documents), "documents", documents)
# classes = intents
print (len(classes), "classes", classes)

"""Creating training data from dataset"""

training = []
output_empty = [0] * len(classes)

# training set, bag of words for each sentence
for doc in documents:
    bag = []
    pattern_words = doc[0]
    # lemmatize each word
    pattern_words = [lemma_function.lemmatize(word.lower()) for word in pattern_words]
    # create our bag of words array with 1, if word match found in current pattern
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)
    
    # output is a '0' for each tag and '1' for current tag (for each pattern)
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    
    training.append([bag, output_row])

random.shuffle(training)
training = np.array(training)

train_x = list(training[:,0])
train_y = list(training[:,1])

"""Building the Neural Network Model"""

# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons
# equal to number of intents to predict output intent with softmax
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

"""Training the model on training data"""

model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)

"""Functions to preprocess input"""

def get_words_from_input_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemma_function.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def get_input_as_bag_of_words(sentence, words, show_details=True):
    sentence_words = get_words_from_input_sentence(sentence)
    bag = [0]*len(words)  
    for s in sentence_words:
        for i,w in enumerate(words):
            if w == s: 
                # assign 1 if current word is in the vocabulary position
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % w)

    return(np.array(bag))

"""Taking input message and converting to bag of words"""

input_sentence = "Which is the nearest pharmacy to me?"

bag_of_words_input = get_input_as_bag_of_words(input_sentence, words)
print ("Input sentence as bag of words - ",bag_of_words_input)
print ("Possible output classes - ",classes)

"""Predict output class using our model"""

inputvar = pd.DataFrame([bag_of_words_input], dtype=float, index=['input'])
model_output = model.predict(inputvar)
print("model output - ", model_output)

print("Input sentence - ", input_sentence)
print("Output class for input - ", classes[model_output.argmax()])

"""Save model and data structures to pickle files"""

pickle.dump(model, open("chatbot-model.pkl", "wb"))

pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( "chatbot-data.pkl", "wb" ) )